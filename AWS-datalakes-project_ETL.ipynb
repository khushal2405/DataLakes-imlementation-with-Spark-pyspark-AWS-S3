{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d08bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1657453377532_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-22-149.ap-south-1.compute.internal:20888/proxy/application_1657453377532_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-22-153.ap-south-1.compute.internal:8042/node/containerlogs/container_1657453377532_0001_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0016404e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Dat, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500dfe33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_song_data(spark,input_data, output_data):\n",
    "    \"\"\"\n",
    "        Description: This function loads song_data from S3 and processes it by extracting the songs and artist tables\n",
    "        and then again loaded back to S3\n",
    "        \n",
    "        Parameters:\n",
    "            spark       : Spark Session\n",
    "            input_data  : location of song_data json files with the songs metadata\n",
    "            output_data : S3 bucket were dimensional tables in parquet format will be stored\n",
    "    \"\"\"\n",
    "    \n",
    "    song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "\n",
    "    songSchema = R([\n",
    "        Fld(\"artist_id\",Str()),\n",
    "        Fld(\"artist_latitude\",Dbl()),\n",
    "        Fld(\"artist_location\",Str()),\n",
    "        Fld(\"artist_longitude\",Dbl()),\n",
    "        Fld(\"artist_name\",Str()),\n",
    "        Fld(\"duration\",Dbl()),\n",
    "        Fld(\"num_songs\",Int()),\n",
    "        Fld(\"title\",Str()),\n",
    "        Fld(\"year\",Int()),\n",
    "    ])\n",
    "    \n",
    "    df = spark.read.json(song_data, schema=songSchema)\n",
    "\n",
    "    song_fields = [\"title\", \"artist_id\",\"year\", \"duration\"]\n",
    "\n",
    "    songs_table = df.select(song_fields).dropDuplicates().withColumn(\"song_id\", monotonically_increasing_id())\n",
    "    \n",
    "    songs_table.write.partitionBy(\"year\", \"artist_id\").parquet(output_data + 'songs/')\n",
    "\n",
    "    artists_fields = [\"artist_id\", \"artist_name as name\", \"artist_location as location\", \"artist_latitude as latitude\", \"artist_longitude as longitude\"]\n",
    "\n",
    "    artists_table = df.selectExpr(artists_fields).dropDuplicates()\n",
    "\n",
    "    artists_table.write.parquet(output_data + 'artists/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1929c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_log_data(spark,input_data, output_data):\n",
    "    \"\"\"\n",
    "        Description: This function loads log_data from S3 and processes it by extracting the songs and artist tables\n",
    "        and then again loaded back to S3. Also output from previous function is used in by spark.read.json command\n",
    "        \n",
    "        Parameters:\n",
    "            spark       : Spark Session\n",
    "            input_data  : location of log_data json files with the events data\n",
    "            output_data : S3 bucket were dimensional tables in parquet format will be stored\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    log_data = input_data + 'log_data/*/*/*.json'\n",
    "\n",
    "    df = spark.read.json(log_data)\n",
    "    \n",
    "    df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "    users_fields = [\"userdId as user_id\", \"firstName as first_name\", \"lastName as last_name\", \"gender\", \"level\"]\n",
    "    users_table = df.selectExpr(users_fields).dropDuplicates()\n",
    "    \n",
    "    users_table.write.parquet(output_data + 'users/')\n",
    "    \n",
    "    get_datetime = udf(date_convert, TimestampType())\n",
    "    df = df.withColumn(\"start_time\", get_datetime('ts'))\n",
    "\n",
    "    time_table = df.select(\"start_time\").dropDuplicates().withColumn(\"hour\", hour(col(\"start_time\"))).withColumn(\"day\", day(col(\"start_time\"))).withColumn(\"week\", week(col(\"start_time\"))).withColumn(\"month\", month(col(\"start_time\"))).withColumn(\"year\", year(col(\"start_time\"))).withColumn(\"weekday\", date_format(col(\"start_time\"), 'E'))\n",
    "                    \n",
    "    \n",
    "    songs_table.write.partitionBy(\"year\", \"month\").parquet(output_data + 'time/')\n",
    "\n",
    "    df_songs = spark.read.parquet(output_data + 'songs/*/*/*')\n",
    "\n",
    "    df_artists = spark.read.parquet(output_data + 'artists/*')\n",
    "\n",
    "    songs_logs = df.join(songs_df, (df.song == songs_df.title))\n",
    "    artists_songs_logs = songs_logs.join(df_artists, (songs_logs.artist == df_artists.name))\n",
    "\n",
    "    songplays = artists_songs_logs.join(\n",
    "        time_table,\n",
    "        artists_songs_logs.ts == time_table.start_time, 'left'\n",
    "    ).drop(artists_songs_logs.year)\n",
    "\n",
    "    songplays_table = songplays.select(\n",
    "        col('start_time').alias('start_time'),\n",
    "        col('userId').alias('user_id'),\n",
    "        col('level').alias('level'),\n",
    "        col('song_id').alias('song_id'),\n",
    "        col('artist_id').alias('artist_id'),\n",
    "        col('sessionId').alias('session_id'),\n",
    "        col('location').alias('location'),\n",
    "        col('userAgent').alias('user_agent'),\n",
    "        col('year').alias('year'),\n",
    "        col('month').alias('month'),\n",
    "    ).repartition(\"year\", \"month\")\n",
    "\n",
    "    songplays_table.write.partitionBy(\"year\", \"month\").parquet(output_data + 'songplays/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9be1f1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "\"cannot resolve '`userdId`' given input columns: [lastName, registration, level, method, ts, location, userAgent, gender, song, itemInSession, length, artist, page, status, sessionId, firstName, userId, auth]; line 1 pos 0;\\n'Project ['userdId AS user_id#278, firstName#244 AS first_name#279, lastName#247 AS last_name#280, gender#245, level#249]\\n+- Filter (page#252 = NextSong)\\n   +- Relation[artist#242,auth#243,firstName#244,gender#245,itemInSession#246L,lastName#247,length#248,level#249,location#250,method#251,page#252,registration#253,sessionId#254L,song#255,status#256L,ts#257L,userAgent#258,userId#259] json\\n\"\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 20, in process_log_data\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1341, in selectExpr\n",
      "    jdf = self._jdf.selectExpr(self._jseq(expr))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`userdId`' given input columns: [lastName, registration, level, method, ts, location, userAgent, gender, song, itemInSession, length, artist, page, status, sessionId, firstName, userId, auth]; line 1 pos 0;\\n'Project ['userdId AS user_id#278, firstName#244 AS first_name#279, lastName#247 AS last_name#280, gender#245, level#249]\\n+- Filter (page#252 = NextSong)\\n   +- Relation[artist#242,auth#243,firstName#244,gender#245,itemInSession#246L,lastName#247,length#248,level#249,location#250,method#251,page#252,registration#253,sessionId#254L,song#255,status#256L,ts#257L,userAgent#258,userId#259] json\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \"\"\"\n",
    "        Extract songs and events data from S3, Transform it into dimensional tables format, and Load it back to S3 in Parquet format\n",
    "    \"\"\"\n",
    "    #spark = create_spark_session()\n",
    "    input_data = \"s3a://kdb-datalakes-project/\"\n",
    "    output_data = \"s3a://kdb-datalakes-project/\"\n",
    "    \n",
    "    #process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "866a6cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1a880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
